---
title: "`r ifelse(exists('title'), title, 'MC Test Analysis')`"
author: "`r ifelse(exists('author'), author, Sys.info()['user'])`"
date: "`r strftime(Sys.time(), '%B %d, %Y')`"
bibliography: mctestanalysis.bib
---

```{r init-opts, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE, results = 'asis')
as_table <- function(x, pander = TRUE, ...) {
  if (pander) {
    pander::pandoc.table(x, split.tables = Inf, ...)
  } else {
    knitr::kable(x, row.names = FALSE, ...)
  }
}

embolden <- function(x, type = 'markdown') {
  switch(
    type,
    'markdown' = paste0('**', x, '**'),
    'html' = paste0('<strong>', x, '</strong>')
  )
}

if (!exists('report_options')) report_options <- list()
stopifnot(exists('mctd'))
mctd <- requires(mctd, c('item.score', 'item.analysis', 'alpha', 'irt_models'))
```

```{r check, eval=FALSE, include=FALSE}
summary_text <- c()

add_to_output <- function(...) {
  element <- paste(...)
  summary_text <<- c(summary_text, element)
}

add_to_output('Questions:')
add_to_output('  - In answer key:', length(mctd$AnswerKey$Question))
add_to_output('  - In test data: ', ncol(mctd$Test))
add_to_output('')
add_to_output('Responses:')
add_to_output('  - Incomplete:', nrow(mctd$Test[!complete.cases(mctd$Test),]))
add_to_output('  - Total:', nrow(mctd$Test))
add_to_output('')
add_to_output('Concepts:', length(unique(mctd$AnswerKey$Concept)))

cat(paste(summary_text, collapse = '\n'))
```

\clearpage

# Introduction

The purpose of this generated report is to provide the analytical framework proposed in the paper 
"An Analytic Framework for Evaluating the Validity of Concept Inventory Claims" [@Jorion2015] 
from the University of Chicago, while providing extra statistical routines based on Classical Test Theory. 
Within the contents of this report, you will find graphical representations such as plots and graphs and tables all intended to validate the 3 claims proposed in Jorionâ€™s paper.

# Test Overview and Descriptions

## Answer Key

```{r answer-key, results = 'asis'}
as_table(mctd$AnswerKey)
```

\clearpage

## Overall Score Histogram

```{r overall-histogram, fig.height=3, fig.width=9}
# Credit: JWilliman @ http://stackoverflow.com/a/36344354
n        <- length(mctd$scores)
mean     <- mean(mctd$scores, na.rm = TRUE)
sd       <- sd(mctd$scores, na.rm = TRUE)
binwidth <- 0.05

data.frame(ids = names(mctd$scores), overall = mctd$scores) %>% 
  ggplot(aes(x = overall)) +
  geom_histogram(bins = 25, color = 'white')+
  stat_function( 
    fun = function(x, mean, sd, n, bw){ 
      dnorm(x = x, mean = mean, sd = sd) * n * bw
    }, 
    args = c(mean = mean, sd = sd, n = n, bw = binwidth),
    color = 'red')+
  xlim(0, 1)+
  theme_minimal()+
  labs(y = "Count", x = "Overall Score")+
  ggtitle("Histogram of Overall Test Scores")
```

## Option Selection by Item

The following table presents the percentage of students selecting each option by item.

```{r options-selected}
optionsSelectedPct(
  mctd,
  include_columns = c('Title', 'Answer', 'Concept'),
  questions_as_row_names = FALSE,
  as_percentage = TRUE,
  correct_vs_incorrect = FALSE
) %>% as_table
```

Table: Option selection by item

# Classic Test Theory

## Summary

The following tables provide common statistical parameters used in Classic Test Theory (CTT).

Cronbach Alpha
:    The coefficient of internal reliability, indicating how closely related the set of items are as a group.

Cronbach Alpha without item (WOI)
:    The Cronbach Alpha calculated for the test without including the item of interest.

Subscale Alpha
:    The Cronbach Alpha for the subscale or concept group. The value of alpha is influenced by test length, so it is expected that a low number of items per subscale will result in a lower subscale alpha value.

Difficulty Index
:    Measures the proportion of students who answered the test item accurately. Higher values close to 1 are indicative of less difficult items (more students answered the item correctly), while lower values close to 0 are associated with more difficult items.

Discrimination Index
:    Measures the ability of the item to discriminate between high and low scoring students. Positive values indicate that the students who scored well on the overall test tended to answer this question correctly, while students who scored poorly on the overall test were likely to answer this question incorrectly. Negative values indicate the opposite -- low-scoring students were more likely to answer the question correctly, while high-scoring students tended to choose the wrong answer -- and suggest that the item should be reviewed. Values near zero suggest the item does not differentiate between high- and low-performing students.

Item Variance
:    Measures the spread among item responses.

Point-Biserial Correlation Coefficient
:    (PBCC) Measures the correlation with the item removed to decrease the influence of the item on the measure of performance.

### Test Summary

```{r ctt-summary}
summarizeCTT(mctd) %>% 
  select(-Measure) %>% 
  as_table(digits = 4, round = 4)
```

Table: Classic Test Theory Summary


### Test Summary by Concept Group

```{r ctt-summary-concept}
as_table(summarizeCTT(mctd, 'concept'), digits = 4, round = 4)
```

Table: Classic Test Theory Summary by Concept Group


### Test Summary by Item

```{r ctt-summary-items}
as_table(summarizeCTT(mctd, 'item'), digits = 4, round = 4)
```

Table: Classic Test Theory Summary by Item


\clearpage

## Discrimination Index

**TEXT NEEDED:** DISCRIMINATION INDEX PLOT TEXT

```{r ctt-discrimination-index, fig.width=12, fig.height=12}
gridExtra::grid.arrange(
  discriminationDifficultyPlot(mctd, "conventional")+ggtitle('Discrimination Index'),
  discriminationDifficultyPlot(mctd, "pbcc")+ggtitle("Point-Biserial Correlation Coefficient"),
  discriminationDifficultyPlot(mctd, "pbcc_modified")+ggtitle("Modified PBCC"),
  ncol = 2
)
```

\clearpage

## Overall Score vs. Question Score

**TEXT NEEDED:** Overall vs Question Score

```{r ctt-overall-vs-question, fig.width = 12, fig.height=12}
testScoreByQuestionPlot(mctd, facet_by_concept = TRUE)
```

\clearpage

# Item Review Recommendations

**TEXT NEEDED:** Item review recommendations

## Review Recommendations Criteria

Alpha
:    If *Cronbach's Alpha* for the test with the item deleted is less than the alpha coefficient for the whole test then the recommendation is to **Keep** the item.

Jorion
:    If the *Difficulty Index* is between 0.3 and 0.9, and the *Discrimination Index* is greater than 0.2, then the recommendation is to **Keep** the item.

Versatile
:    This recommendation is based on the *Difficulty Index* and *PBCC* and provides a range of recommendations from **Remove** to **Review** through **Keep**, favoring positive PBCC values near to or greater than 0.3 and higher difficulty values.

Stringent
:    If the *Difficulty Index* is between 0.3 and 0.9, and the *Point-Biserial Correlation Coefficient* is greater than 0.3, then the recommendation is to **Keep** the item.


## Review Recommendations Table

```{r ctt-review-recommendations}
recommendItemActions(mctd, include_columns = c("Title", "Concept"), digits.round = 2) %>% as_table()
```

Table: Recommendations for each test item based on the criteria described above.

\clearpage

# Item Response Theory

```{r irt-best-model, include=FALSE}
if ('irt_model_choice' %in% names(report_options)) {
  pl_number <- report_options$irt_model_choice %>% as.integer
  flag_model_chosen <- TRUE
} else {
  flag_model_chosen <- FALSE
  pl_number <- which(mctd$irt_models$AIC == min(mctd$irt_models$AIC)) %>% 
    names() %>% substr(start = 3, stop = 3) %>% as.integer
}
number_words <- c('one', 'two', 'three')
pl_name <- paste0('PL', pl_number)
```

## Model Summary

`r ifelse(flag_model_chosen, 'The model selected by the user for', "One-, two- and three-parameter logistic models were fit to the test results data. The model chosen for the remained of")` 
this analysis was the
`r number_words[pl_number]`-factor logistic model,
which had 
`r ifelse(flag_model_chosen, 'an', 'the lowest')`
AIC of 
$`r mctd$irt_models$AIC[pl_number] %>% round(1)`$.

**Model Parameters**

```{r irt-model-param-text}
irt_help_text <- list(
  paste(
    "Difficulty\n:   ",
    "The difficulty parameter, \\(\\beta\\), sometimes",
    "called the threshold parameter, describes the difficulty of a given item.",
    "It is the only parameter estimated in the 1-PL (Rasch) model.\n\n"
  ),
  paste(
    "Discrimination\n:   ",
    "The discrimination parameter, \\(\\alpha\\),",
    "reflects the effectiveness of the item in differentiating between high- and",
    "low-performing students. This parameter is estimated in the 2-PL model, in",
    "addition to difficulty.\n\n"
  ),
  paste(
    "Guessing\n:   ",
    "The guessing parameter, \\(\\gamma\\), is included in the",
    "3-PL model, in addition the previous parameters, and reflects the influence",
    "of guessing for each item.\n\n"
  ),
  paste(
    "Prob.\n:   ",
    "The probability column gives the probability that an average",
    "student will correctly answer the item, i.e.",
    "\\(\\mathrm{P}(x_i = 1 \\vert z = 0)\\).\n\n"
  ),
  # Discrimination description for Rasch model
  paste(
    "Discrimination\n:  ",
    "In the 1-PL Rasch model, the discrimination parameter is assumed to be equivalent across all items.",
    "This assumption leads to consistent ICC curves where more difficult questions are always less easy",
    "for all students. When the discrimination parameter is allowed to vary, for two items of similar",
    "difficulty one item can be both easier for low-performing students and harder for high-performing",
    "students when compared with the second item (or vice-versa).\n\n"
  )
)
# Print out IRT model parameter text
do.call('cat', irt_help_text[c(1:pl_number, if(pl_number == 1) 5, 4)])
```

```{r irt-summary, echo=FALSE}
irtSummaryTable(mctd, pl_number) %>% as_table()
```

## Item Characteristic Curves

```{r icc-curves, echo=FALSE, fig.height=6, fig.width=12}
for (concept in unique(mctd$AnswerKey$Concept)) {
  cat("\n\n###", concept, "\n\n")
  questions <- mctd$AnswerKey %>%
    mutate(n = 1:nrow(.)) %>%
    filter(Concept == concept) %>% .$n
  plot_title <- paste0("Item Characteristic Curves: Concept \"", concept, '"')
  switch(pl_name,
         'PL1' = ltm::plot.rasch(mctd$irt_models[['PL1']], 
                                 type = "ICC", items = questions,
                                 main = plot_title),
         'PL2' = ltm::plot.ltm(mctd$irt_models[['PL2']], 
                                 type = "ICC", items = questions,
                                 main = plot_title),
         'PL3' = ltm::plot.tpm(mctd$irt_models[['PL3']], 
                                 type = "ICC", items = questions,
                                 main = plot_title)
  )
}
```

\clearpage

# Factor Analysis

## Tetrachoric Plot

```{r tetrachoric-plot, fig.height=9, fig.width=9, dev='png'}
plotTetrachoric(mctd, TRUE, TRUE)
```

## Scree Plot

```{r scree-plot}
scree_factors <- screePlot(mctd, TRUE)
```

A method for determining the number of factors or components in the tetrachoric correlation matrix of the test responses is to examine the scree plot of the eigenvalues of the correlation matrix.
Typically, when using a scree plot, the analist is looking for a sharp break in the slope of the line between the eigenvalues of the correlation matrix.
In parallel analysis, the scree of factors from the observed data is compared to that of a random data matrix of the same size as the observed.
Parallel analysis suggests a number of factors/components by comparing the eigenvalues of the factors/components of the observed data to the random data and keeping those that are greater than the random data.

Parallel analysis for the test results in this report suggest that the number of factors is
`r scree_factors['nfact']`
and the number of components is
`r scree_factors['ncomp']`.

## Exploratory Factor Analysis

```{r efa-options, include = FALSE}
flag_nfactors_chosen_by_user <- FALSE
if ('efa.nfactors' %in% names(report_options)) {
  flag_nfactors_chosen_by_user <- TRUE
  efa.nfactors <- report_options$efa.nfactors
  if (efa.nfactors == 0) efa.nfactors <- length(unique(mctd$AnswerKey$Concept))
  else if (efa.nfactors == -1) efa.nfactors <- scree_factors['nfact']
} else efa.nfactors <- length(unique(mctd$AnswerKey$Concept))

efa.cut <- ifelse('efa.cut' %in% names(report_options), report_options$efa.cut, 0.3)
efa.n.obs <- nrow(mctd$Test.complete)
efa.rotate <- ifelse('efa.rotate' %in% names(report_options), report_options$efa.rotate, 'varimax')
efa.rotate.text <- paste0("`'", efa.rotate, "'`")
efa.fm <- ifelse('efa.fm' %in% names(report_options), report_options$efa.fm, 'minres')
efa.fm.text <- paste0("`'", efa.fm, "'`")
```

The table below presents the factor loadings, where
`r efa.nfactors`
were explored, using the `fa()` function from the [`psych` package](https://cran.r-project.org/web/packages/psych/) (see @Psych2016 for more information on the options available for this function).
In this report, the EFA used the
`r efa.rotate.text` rotation method and the 
`r efa.fm.text` factoring method.
Factors with absolute value loadings less than
`r efa.cut`
were suppressed.


```{r efa}
efaTable(mctd,
         cut = efa.cut,
         nfactors = efa.nfactors, 
         n.obs = efa.n.obs,
         rotate = efa.rotate,
         fm = efa.fm) %>% 
  as_table(missing = '')
```

\clearpage

# Distractor Analysis

```{r distractor-table-data, include = FALSE}
if ('distractor.pct' %in% names(report_options)) {
  distractor.pct <- report_options$distractor.pct
} else distractor.pct <- 0.33

distractor.data <- distractorTable(mctd, distractor.pct)
distractor.data.counts <- distractor.data %>% 
  filter(Question == distractor.data[1, 'Question']) %>% 
  group_by(Group) %>% 
  summarize(total = sum(count))

# For turning percentile into words
percentile_abreviation <- c('th', 'st', 'nd', 'rd', rep('th', 6))
first_digit <- function(x) round(x %% 10, 0)
pct_to_text <- function(x) {
  paste0(round(x * 100, 0), percentile_abreviation[first_digit(x*100) + 1])
}
```

The following plot and table compare the percentage of all respondents who select a given option for each item.
These tables allow the test administrator to analize the performance of item options and to determine if the choice of distracting items reveals information about the misconceptions in students' knowledge.
Repondents are grouped into the upper and lower
`r pct_to_text(distractor.pct)`
percentiles by overall test score.
For this report, there were
`r distractor.data.counts %>% filter(Group == 'high') %>% .$total`
respondents in the upper
`r pct_to_text(distractor.pct)` percentile and
`r distractor.data.counts %>% filter(Group == 'low') %>% .$total`
repondents in the lower
`r pct_to_text(distractor.pct)` percentile.
Percentages are calculated relative to the total number of respondents, in this case
`r nrow(mctd$Test.complete)`
students.

```{r distractor-plot, fig.height=8, fig.width=12}
distractorPlot(mctd, distractor.pct) + 
  ggtitle('Distractor Analysis')
```

```{r distractor-table}
n_options <- length(unique(distractor.data$Option))
col_justifications <- paste(paste0(rep('c', 2), collapse = ''), paste0(rep('r', n_options*2), collapse = ''), sep = '')
distractor.data %>% 
  mutate(pct = sprintf("%0.2f", pct*100), 
         pct = ifelse(Correct, embolden(pct), pct), 
         Group = c('high' = 'H', 'low' = 'L')[Group], 
         OptionGroup = paste(Option, Group, sep = '')) %>% 
  select(Question, Title, OptionGroup, pct) %>% 
  reshape2::dcast(Question + Title ~ OptionGroup, value.var = 'pct') %>% 
  as_table(justify = col_justifications)
```

Table: Percentage of total respondents ($N=$ `r nrow(mctd$Test.complete)`) from upper (*High*, $N=$ `r distractor.data.counts %>% filter(Group == 'high') %>% .$total`) and lower (*Low*, $N=$ `r distractor.data.counts %>% filter(Group == 'low') %>% .$total`) `r pct_to_text(distractor.pct)` percentiles having chosen each item option. The percentage of students choosing the correct option for each item are highlighted in bold.

# References
